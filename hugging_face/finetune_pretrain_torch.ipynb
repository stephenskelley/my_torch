{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d58380b-3ce6-4688-b50e-f920c85603d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from pytorch_lightning import LightningModule, Trainer, callbacks\n",
    "import evaluate\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f96ec955-9505-4c4e-8ff0-5eae506dc693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset yelp_review_full (/Users/skelley/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6957973edde0460e80dd60f7e972261b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/skelley/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-1b229fb576f6410a.arrow\n",
      "Loading cached processed dataset at /Users/skelley/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-769d3116751632a1.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/skelley/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-6e4b596798578d8f.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/skelley/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-219ca94af128ec87.arrow\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# get dataset in appropriate format for pytorch\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "# use small sample of full dataset\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(5120))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(512))\n",
    "# data loaders\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fea0407b-c98a-43b1-b9f0-17668ced5ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLightning(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\", output_attentions=True)\n",
    "        self.W = torch.nn.Linear(self.bert.config.hidden_size, 5)\n",
    "        self.num_classes = 5\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        result = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = self.W(result['last_hidden_state'][:, 0])\n",
    "        return logits\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=5e-5)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y, input_ids, token_type_ids, attention_mask = batch['labels'], batch['input_ids'], batch['token_type_ids'], batch['attention_mask']\n",
    "        pred = self(input_ids, token_type_ids, attention_mask)\n",
    "        loss = self.loss_function(pred, y)\n",
    "        accuracy = sum(pred.argmax(1) == y)/len(y)\n",
    "        self.log(\"training_loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.log(\"training_accuracy\", accuracy, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y, input_ids, token_type_ids, attention_mask = batch['labels'], batch['input_ids'], batch['token_type_ids'], batch['attention_mask']\n",
    "        pred = self(input_ids, token_type_ids, attention_mask)\n",
    "        loss = self.loss_function(pred, y)\n",
    "        accuracy = sum(pred.argmax(1) == y)/len(y)\n",
    "        self.log(\"validation_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log(\"validation_accuracy\", accuracy, prog_bar=True, on_step=True, on_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dde276c5-a383-41ad-8158-468bedd6a4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | bert          | BertModel        | 108 M \n",
      "1 | W             | Linear           | 3.8 K \n",
      "2 | loss_function | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "433.256   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969b8cfbbc6c4a20aedac8cfc648d545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = callbacks.ModelCheckpoint(dirpath='./bert_lightning/',filename='bert_{epoch}')\n",
    "model = BertLightning()\n",
    "trainer = Trainer(max_epochs=3, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=eval_dataloader)\n",
    "torch.save(model.state_dict(),'./bert_lightning/weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7a8e733-4d95-4d4d-84d0-e3d08a4a41be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = BertLightning()\n",
    "loaded_model.load_from_checkpoint('./bert_lightning/bert_epoch=2.ckpt')\n",
    "loaded_model.load_state_dict(torch.load('./bert_lightning/weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc4e2d36-338a-48a2-aef7-14168fff0548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4, 1, 4, 3, 4, 2, 3, 2, 3, 0, 0, 3, 2, 2, 1]) tensor([[  101, 14812, 16442,  ...,     0,     0,     0],\n",
      "        [  101, 19383,  1303,  ...,     0,     0,     0],\n",
      "        [  101, 12008, 27788,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1753,  1277,  ...,     0,     0,     0],\n",
      "        [  101, 13377,   112,  ...,     0,     0,     0],\n",
      "        [  101,   140,  3161,  ...,     0,     0,     0]]) tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]) tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([2, 4, 1, 4, 3, 4, 2, 3, 2, 3, 0, 0, 3, 2, 2, 1])\n",
      "tensor([[-2.3139e+00, -2.7456e+00, -3.5378e-01,  2.5809e+00,  2.5276e+00],\n",
      "        [-4.0275e+00, -2.2898e+00,  3.4590e+00,  4.4969e+00, -2.6675e-01],\n",
      "        [ 2.1004e+00,  2.5073e+00, -1.3661e-01, -1.6072e+00, -2.4533e+00],\n",
      "        [-3.7632e+00, -3.7796e+00,  7.1251e-01,  4.7825e+00,  2.4308e+00],\n",
      "        [-3.7639e+00, -2.6468e+00,  2.4347e+00,  4.2633e+00,  9.8429e-01],\n",
      "        [-2.9427e+00, -3.1270e+00, -7.7332e-04,  2.9713e+00,  3.3659e+00],\n",
      "        [ 1.7337e+00,  1.7952e+00,  1.7184e-01, -2.0380e+00, -2.3989e+00],\n",
      "        [-4.0677e+00, -2.3048e+00,  2.5241e+00,  4.3965e+00,  5.3772e-01],\n",
      "        [-2.6451e+00, -2.8750e+00, -1.3552e-01,  2.5546e+00,  3.2865e+00],\n",
      "        [-3.3295e+00, -2.4499e+00,  1.9462e+00,  3.3636e+00,  1.2486e+00],\n",
      "        [-8.3821e-01,  3.9154e+00,  3.0825e+00, -1.5422e+00, -3.0592e+00],\n",
      "        [ 4.8833e+00,  3.7223e+00, -1.0023e+00, -3.6128e+00, -3.7694e+00],\n",
      "        [-3.3977e+00, -3.7296e+00,  1.0258e-01,  3.8160e+00,  3.3267e+00],\n",
      "        [-3.6212e+00,  2.8815e-01,  5.4074e+00,  2.2312e+00, -2.5210e+00],\n",
      "        [-2.4285e+00,  2.1167e+00,  4.8665e+00,  2.3785e-01, -3.3710e+00],\n",
      "        [-2.9222e+00, -5.7854e-01,  2.4777e+00,  2.5240e+00, -5.9511e-01]])\n",
      "tensor(1.8124)\n",
      "tensor([3, 3, 1, 3, 3, 4, 1, 3, 4, 3, 1, 0, 3, 2, 2, 3])\n",
      "{'accuracy': 0.5625}\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = numpy.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "it = iter(eval_dataloader)\n",
    "batch  = next(it)\n",
    "labels, input_ids, token_type_ids, attention_mask = batch['labels'], batch['input_ids'], batch['token_type_ids'], batch['attention_mask']\n",
    "print(labels, input_ids, token_type_ids, attention_mask)\n",
    "with torch.no_grad():\n",
    "    result =  loaded_model.forward(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    print(labels)\n",
    "    print(result)\n",
    "    print(loss_function(result, labels))\n",
    "    print(numpy.argmax(result, axis=-1))\n",
    "    print(compute_metrics((result, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ae27c19-4a65-4568-ad23-d850df5a1df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 9.0 9.0\n",
      "32 9.0 18.0\n",
      "48 7.0 25.0\n",
      "64 13.0 38.0\n",
      "80 10.0 48.0\n",
      "96 5.0 53.0\n",
      "112 10.0 63.0\n",
      "128 12.0 75.0\n",
      "144 7.0 82.0\n",
      "160 12.0 94.0\n",
      "176 12.0 106.0\n",
      "192 7.0 113.0\n",
      "208 10.0 123.0\n",
      "224 9.0 132.0\n",
      "240 9.0 141.0\n",
      "256 8.0 149.0\n",
      "272 9.0 158.0\n",
      "288 9.0 167.0\n",
      "304 10.0 177.0\n",
      "320 12.0 189.0\n",
      "336 11.0 200.0\n",
      "352 8.0 208.0\n",
      "368 9.0 217.0\n",
      "384 6.0 223.0\n",
      "400 10.0 233.0\n",
      "416 10.0 243.0\n",
      "432 10.0 253.0\n",
      "448 12.0 265.0\n",
      "464 7.0 272.0\n",
      "480 11.0 283.0\n",
      "496 7.0 290.0\n",
      "512 5.0 295.0\n",
      "0.576171875\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "correct = 0\n",
    "for step, batch in enumerate(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "        labels, input_ids, token_type_ids, attention_mask = batch['labels'], batch['input_ids'], batch['token_type_ids'], batch['attention_mask']\n",
    "        predictions = loaded_model(input_ids, token_type_ids, attention_mask)\n",
    "        new_correct = len(labels) * compute_metrics((predictions, labels))['accuracy']\n",
    "        correct = correct + new_correct\n",
    "        count = count + len(labels)\n",
    "        print(count, new_correct, correct)\n",
    "print(correct/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "840e1647-a5df-439a-b94d-e52d732b6c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5103, dtype=torch.float64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.tensor([[-1e-6, -1e-6, 1e-6], [1e-6,-1e-6,-1e-6]], dtype=torch.float64)\n",
    "labels = torch.tensor([2,0])\n",
    "result = torch.tensor([[0, 1.1, 0]], dtype=torch.float64)\n",
    "labels = torch.tensor([1])\n",
    "loss_function(result, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61c8d7a4-c176-4e42-91a4-ac335b4fa4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0.]])\n",
      "tensor([1])\n",
      "tensor(0.5514)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "out = torch.tensor ([[0.0, 1.0, 0.0]])\n",
    "out_logits = torch.tensor ([[-1.e6, 1.e6, -1.e6]])\n",
    "print(torch.softmax (out_logits, 1))\n",
    "target = torch.argmax (out, dim = 1)\n",
    "target = torch.tensor([1])\n",
    "print(target)\n",
    "print(torch.nn.CrossEntropyLoss() (out, target))\n",
    "print(torch.nn.CrossEntropyLoss() (out_logits, target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88825c-876b-4324-a887-093d47d99873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
