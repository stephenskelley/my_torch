{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58380b-3ce6-4688-b50e-f920c85603d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from pytorch_lightning import LightningModule, Trainer, callbacks\n",
    "import evaluate\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ec955-9505-4c4e-8ff0-5eae506dc693",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# get dataset in appropriate format for pytorch\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "# use small sample of full dataset\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(5120))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(512))\n",
    "# data loaders\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0407b-c98a-43b1-b9f0-17668ced5ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLightning(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\", output_attentions=True)\n",
    "        self.W = torch.nn.Linear(self.bert.config.hidden_size, 5)\n",
    "        self.num_classes = 5\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        result = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = self.W(result['last_hidden_state'][:, 0])\n",
    "        return logits\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=5e-5)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y, input_ids, token_type_ids, attention_mask = batch['labels'], batch['input_ids'], batch['token_type_ids'], batch['attention_mask']\n",
    "        pred = self(input_ids, token_type_ids, attention_mask)\n",
    "        loss = self.loss_function(pred, y)\n",
    "        accuracy = sum(pred.argmax(1) == y)/len(y)\n",
    "        self.log(\"training_loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.log(\"training_accuracy\", accuracy, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y, input_ids, token_type_ids, attention_mask = batch['labels'], batch['input_ids'], batch['token_type_ids'], batch['attention_mask']\n",
    "        pred = self(input_ids, token_type_ids, attention_mask)\n",
    "        loss = self.loss_function(pred, y)\n",
    "        accuracy = sum(pred.argmax(1) == y)/len(y)\n",
    "        self.log(\"validation_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log(\"validation_accuracy\", accuracy, prog_bar=True, on_step=True, on_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde276c5-a383-41ad-8158-468bedd6a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = callbacks.ModelCheckpoint(dirpath='./bert_lightning/',filename='bert_{epoch}')\n",
    "model = BertLightning()\n",
    "trainer = Trainer(max_epochs=3, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=eval_dataloader)\n",
    "torch.save(model.state_dict(),'./bert_lightning/weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8e733-4d95-4d4d-84d0-e3d08a4a41be",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = BertLightning()\n",
    "loaded_model.load_from_checkpoint('./bert_lightning/bert_epoch=2.ckpt')\n",
    "loaded_model.load_state_dict(torch.load('./bert_lightning/weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e2d36-338a-48a2-aef7-14168fff0548",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = numpy.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "it = iter(eval_dataloader)\n",
    "batch  = next(it)\n",
    "labels, input_ids, token_type_ids, attention_mask = batch['labels'], batch['input_ids'], batch['token_type_ids'], batch['attention_mask']\n",
    "print(labels, input_ids, token_type_ids, attention_mask)\n",
    "with torch.no_grad():\n",
    "    result =  loaded_model.forward(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    print(labels)\n",
    "    print(result)\n",
    "    print(loss_function(result, labels))\n",
    "    print(numpy.argmax(result, axis=-1))\n",
    "    print(compute_metrics((result, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae27c19-4a65-4568-ad23-d850df5a1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "correct = 0\n",
    "for step, batch in enumerate(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "        labels, input_ids, token_type_ids, attention_mask = batch['labels'], batch['input_ids'], batch['token_type_ids'], batch['attention_mask']\n",
    "        predictions = loaded_model(input_ids, token_type_ids, attention_mask)\n",
    "        new_correct = len(labels) * compute_metrics((predictions, labels))['accuracy']\n",
    "        correct = correct + new_correct\n",
    "        count = count + len(labels)\n",
    "        print(count, new_correct, correct)\n",
    "print(correct/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88825c-876b-4324-a887-093d47d99873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
