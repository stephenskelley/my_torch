{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d53c8b07-c29c-4daa-b89a-fb7c440944b1",
   "metadata": {},
   "source": [
    "https://gpt-index.readthedocs.io/en/latest/how_to/using_with_langchain.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db92b705-8a07-4ef0-954f-41d96c31d6d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'index_struct'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlocal_secrets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msecrets\u001b[39;00m\n\u001b[1;32m      9\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m secrets\u001b[39m.\u001b[39mtechstyle_openai_key\n\u001b[0;32m---> 10\u001b[0m ds_techrithm_index \u001b[39m=\u001b[39m  GPTSimpleVectorIndex\u001b[39m.\u001b[39;49mload_from_disk(\u001b[39m'\u001b[39;49m\u001b[39m./stash_index/DATASCIENCE_techrithm.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m tool_config \u001b[39m=\u001b[39m IndexToolConfig(\n\u001b[1;32m     13\u001b[0m     index\u001b[39m=\u001b[39mds_techrithm_index, \n\u001b[1;32m     14\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mVector Index\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     tool_kwargs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mreturn_direct\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m}\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m tool \u001b[39m=\u001b[39m LlamaIndexTool\u001b[39m.\u001b[39mfrom_tool_config(tool_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_torch/lib/python3.10/site-packages/llama_index/indices/base.py:352\u001b[0m, in \u001b[0;36mBaseGPTIndex.load_from_disk\u001b[0;34m(cls, save_path, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(save_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    351\u001b[0m     file_contents \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 352\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload_from_string(file_contents, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_torch/lib/python3.10/site-packages/llama_index/indices/base.py:328\u001b[0m, in \u001b[0;36mBaseGPTIndex.load_from_string\u001b[0;34m(cls, index_string, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load index from string (in JSON-format).\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[1;32m    311\u001b[0m \u001b[39mThis method loads the index from a JSON string. The index data\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m \n\u001b[1;32m    326\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m result_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(index_string)\n\u001b[0;32m--> 328\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload_from_dict(result_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_torch/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py:260\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex.load_from_dict\u001b[0;34m(cls, result_dict, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m VECTOR_STORE_CONFIG_DICT_KEY \u001b[39min\u001b[39;00m result_dict:\n\u001b[1;32m    259\u001b[0m     config_dict \u001b[39m=\u001b[39m result_dict[VECTOR_STORE_CONFIG_DICT_KEY]\n\u001b[0;32m--> 260\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mload_from_dict(result_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_torch/lib/python3.10/site-packages/llama_index/indices/base.py:302\u001b[0m, in \u001b[0;36mBaseGPTIndex.load_from_dict\u001b[0;34m(cls, result_dict, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39m# NOTE: lazy load registry\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mindices\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistry\u001b[39;00m \u001b[39mimport\u001b[39;00m load_index_struct_from_dict\n\u001b[0;32m--> 302\u001b[0m index_struct \u001b[39m=\u001b[39m load_index_struct_from_dict(result_dict[INDEX_STRUCT_KEY])\n\u001b[1;32m    303\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(index_struct, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mindex_struct_cls)\n\u001b[1;32m    304\u001b[0m docstore \u001b[39m=\u001b[39m DocumentStore\u001b[39m.\u001b[39mload_from_dict(result_dict[DOCSTORE_KEY])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'index_struct'"
     ]
    }
   ],
   "source": [
    "from gpt_index.langchain_helpers.agents import IndexToolConfig, LlamaIndexTool\n",
    "from llama_index import QuestionAnswerPrompt, GPTSimpleVectorIndex, GPTListIndex, LLMPredictor\n",
    "from langchain import OpenAI\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import local_secrets as secrets\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = secrets.techstyle_openai_key\n",
    "ds_techrithm_index =  GPTSimpleVectorIndex.load_from_disk('./stash_index/DATASCIENCE_techrithm.json')\n",
    "\n",
    "tool_config = IndexToolConfig(\n",
    "    index=ds_techrithm_index, \n",
    "    name=f\"Vector Index\",\n",
    "    description=f\"useful for when you want to answer queries about X\",\n",
    "    index_query_kwargs={\"similarity_top_k\": 3},\n",
    "    tool_kwargs={\"return_direct\": True}\n",
    ")\n",
    "\n",
    "tool = LlamaIndexTool.from_tool_config(tool_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c1c86-e23b-4855-83f8-3c46619f2975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
