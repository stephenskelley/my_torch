{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22cdc707-c2a1-42c8-b1d5-62773c67e0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import QuestionAnswerPrompt, GPTSimpleVectorIndex, GPTListIndex, LLMPredictor\n",
    "from langchain import OpenAI\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import local_secrets as secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc420d49-1e01-40c1-a255-20ee3301ae34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'index_struct'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m secrets\u001b[39m.\u001b[39mtechstyle_openai_key\n\u001b[1;32m      2\u001b[0m index_files \u001b[39m=\u001b[39m [file \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(\u001b[39m'\u001b[39m\u001b[39m./stash_index\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mif\u001b[39;00m file\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mDATASCIENCE\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m----> 3\u001b[0m indexes \u001b[39m=\u001b[39m [GPTSimpleVectorIndex\u001b[39m.\u001b[39mload_from_disk(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./stash_index/\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m index_files]\n\u001b[1;32m      4\u001b[0m [index\u001b[39m.\u001b[39mset_text(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstash \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m repository\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m (file,index) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(index_files, indexes)]\n\u001b[1;32m      5\u001b[0m stash_ds_index \u001b[39m=\u001b[39m GPTListIndex(indexes)\n",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m secrets\u001b[39m.\u001b[39mtechstyle_openai_key\n\u001b[1;32m      2\u001b[0m index_files \u001b[39m=\u001b[39m [file \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(\u001b[39m'\u001b[39m\u001b[39m./stash_index\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mif\u001b[39;00m file\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mDATASCIENCE\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m----> 3\u001b[0m indexes \u001b[39m=\u001b[39m [GPTSimpleVectorIndex\u001b[39m.\u001b[39;49mload_from_disk(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./stash_index/\u001b[39;49m\u001b[39m{\u001b[39;49;00mfile\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m) \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m index_files]\n\u001b[1;32m      4\u001b[0m [index\u001b[39m.\u001b[39mset_text(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstash \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m repository\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m (file,index) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(index_files, indexes)]\n\u001b[1;32m      5\u001b[0m stash_ds_index \u001b[39m=\u001b[39m GPTListIndex(indexes)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_torch/lib/python3.10/site-packages/llama_index/indices/base.py:352\u001b[0m, in \u001b[0;36mBaseGPTIndex.load_from_disk\u001b[0;34m(cls, save_path, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(save_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    351\u001b[0m     file_contents \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 352\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload_from_string(file_contents, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_torch/lib/python3.10/site-packages/llama_index/indices/base.py:328\u001b[0m, in \u001b[0;36mBaseGPTIndex.load_from_string\u001b[0;34m(cls, index_string, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load index from string (in JSON-format).\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[1;32m    311\u001b[0m \u001b[39mThis method loads the index from a JSON string. The index data\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m \n\u001b[1;32m    326\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m result_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(index_string)\n\u001b[0;32m--> 328\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload_from_dict(result_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_torch/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py:260\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex.load_from_dict\u001b[0;34m(cls, result_dict, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m VECTOR_STORE_CONFIG_DICT_KEY \u001b[39min\u001b[39;00m result_dict:\n\u001b[1;32m    259\u001b[0m     config_dict \u001b[39m=\u001b[39m result_dict[VECTOR_STORE_CONFIG_DICT_KEY]\n\u001b[0;32m--> 260\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mload_from_dict(result_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_torch/lib/python3.10/site-packages/llama_index/indices/base.py:302\u001b[0m, in \u001b[0;36mBaseGPTIndex.load_from_dict\u001b[0;34m(cls, result_dict, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39m# NOTE: lazy load registry\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mindices\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistry\u001b[39;00m \u001b[39mimport\u001b[39;00m load_index_struct_from_dict\n\u001b[0;32m--> 302\u001b[0m index_struct \u001b[39m=\u001b[39m load_index_struct_from_dict(result_dict[INDEX_STRUCT_KEY])\n\u001b[1;32m    303\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(index_struct, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mindex_struct_cls)\n\u001b[1;32m    304\u001b[0m docstore \u001b[39m=\u001b[39m DocumentStore\u001b[39m.\u001b[39mload_from_dict(result_dict[DOCSTORE_KEY])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'index_struct'"
     ]
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'] = secrets.techstyle_openai_key\n",
    "index_files = [file for file in os.listdir('./stash_index') if file.startswith('DATASCIENCE')]\n",
    "indexes = [GPTSimpleVectorIndex.load_from_disk(f'./stash_index/{file}') for file in index_files]\n",
    "[index.set_text(f\"stash {file.replace('_', ' ')} repository\") for (file,index) in zip(index_files, indexes)]\n",
    "stash_ds_index = GPTListIndex(indexes)\n",
    "stash_ds_index.set_text('stash data science repository')\n",
    "brand_analytics_index = GPTSimpleVectorIndex.load_from_disk('./stash_index/EDW_brand-analytics.json')\n",
    "ds_techrithm_index =  GPTSimpleVectorIndex.load_from_disk('./stash_index/DATASCIENCE_techrithm.json')\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(max_tokens=512))\n",
    "QA_PROMPT_TMPL = (\n",
    "    \"We have provided context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this information, please perform the following: {query_str}\\n\"\n",
    ")\n",
    "QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd046785-9465-484b-975c-46ba0618fe28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 361 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 15 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import local_secrets as secrets\n",
      "from algorithm_library.product_algorithm import ProductAlgorithm\n",
      "\n",
      "algo = ProductAlgorithm('ShoeDazzle', 'All', secrets)\n",
      "df = algo.snowflake_query_to_pandas(algo.queries.q_spid_category())\n",
      "\n",
      "# Get all ShoeDazzle master product ids\n",
      "shoedazzle_master_product_ids = df[df['store_group'] == 'ShoeDazzle']['master_product_id'].unique()\n",
      "print(shoedazzle_master_product_ids)\n"
     ]
    }
   ],
   "source": [
    "query_str = \"Write code to to get all ShoeDazzle master product id's\"\n",
    "print(ds_techrithm_index.query(query_str, text_qa_template=QA_PROMPT, mode='embedding', llm_predictor=llm_predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3acdced-1e7e-445e-ab07-0524eb20c678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 681 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 13 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT SUM(fso.cash_gross_revenue) as cash_gross_revenue\n",
      "FROM edw.analytics_base.finance_sales_ops fso\n",
      "JOIN edw.data_model.dim_store s on s.store_id = fso.store_id\n",
      "WHERE store_brand = 'Fabletics'\n",
      "AND date_object = 'Placed'\n",
      "AND gender = 'F'\n",
      "AND store_type IN ('Online','Mobile App')\n",
      "AND store_region = 'NA'\n",
      "AND TO_CHAR(fso.date, 'YYYY') = '2022'\n",
      "GROUP BY month\n",
      "ORDER BY month;\n"
     ]
    }
   ],
   "source": [
    "query_str = \"Write a simple query to get Fabletics gross revenue in 2022\"\n",
    "print(brand_analytics_index.query(query_str, text_qa_template=QA_PROMPT, mode='embedding', llm_predictor=llm_predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfb7eca9-342c-46cd-bb38-4e17fc004f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 597 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 14 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "select sum(product_net_revenue) as product_net_rev \n",
      "from edw.analytics_base.finance_Sales_ops fso\n",
      "join edw.data_model.dim_customer c on c.customer_id = fso.customer_id\n",
      "join edw.data_model.dim_store st on st.store_id = fso.store_id\n",
      "join edw.data_model.dim_order_membership_classification omc on omc.order_membership_classification_key = fso.order_membership_classification_key\n",
      "where st.store_name = 'Fabletics UK'\n",
      "and st.store_type != 'Retail'\n",
      "and currency_object = 'usd'\n",
      "and date_object = 'placed'\n",
      "and date >= '2022-01-01'\n",
      "and date <= '2022-12-31'\n",
      ";\n"
     ]
    }
   ],
   "source": [
    "query_str = \"Write a simple query to get Fabletics UK net revenue in 2022\"\n",
    "print(brand_analytics_index.query(query_str, text_qa_template=QA_PROMPT, mode='embedding', llm_predictor=llm_predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f92eeb4-dabd-4b26-8bc5-f59ae8156bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
