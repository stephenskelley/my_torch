{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running PyTorch on MPS requires MacOS 12.3+ *and* the ARM version of Python installed. We can check the MacOS version with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform; platform.mac_ver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first item tells us our MacOS version, this must be greater than *12.3* and if it is not then update your Mac!\n",
    "\n",
    "The final item tells us the OS version our current environment is running in, this should be *arm64*. An alternative check is to print the `platform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform.platform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above displays something like `macOS-12.4-x86_64-i386-64bit` (eg containing `x86`), we have the wrong version of Python installed and must install the correct (ARM) version. If using Anaconda a new ARM environment can be set up like so:\n",
    "\n",
    "```bash\n",
    "CONDA_SUBDIR=osx-arm64 conda create -n ml python=3.9 -c conda-forge\n",
    "```\n",
    "\n",
    "Here we are setting the conda version variable to use ARM versions for install. We then `create` a new `conda` environment with name (`-n`) `ml`. We use Python `3.9` for this and make sure we have `conda-forge` as a repository in our channels `-c` where these ARM installs can be downloaded from.\n",
    "\n",
    "Next we activate the environment with `conda activate ml` and modify the `CONDA_SUBDIR` variable to permanently use `osx-arm64`, otherwise this may default back to an incorrect *x84* version during future installs.\n",
    "\n",
    "```bash\n",
    "conda env config vars set CONDA_SUBDIR=osx-arm64\n",
    "```\n",
    "\n",
    "You may see a message asking you to reactivate the environment for these changes to take effect, if so just switch back to the *base* environment then back into the `ml` environment with:\n",
    "\n",
    "```bash\n",
    "conda activate\n",
    "conda activate ml\n",
    "```\n",
    "\n",
    "Now we're ready to install the latest PyTorch version (v1.12 or higher), at the moment this requires that we install the PyTorch nightly preview with:\n",
    "\n",
    "```bash\n",
    "pip install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "```\n",
    "\n",
    "During downloads you should be able to see something like *Downloading torch-1.1x.x.---**arm64.whl***. That final *arm64.whl* part is important and tells us we are downloading the correct version.\n",
    "\n",
    "For the examples in this notebook we will also use HF *transformers* and *datasets*.\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets\n",
    "```\n",
    "\n",
    "---\n",
    "***Note**: The transformers library uses tokenizers built in Rust (it makes them faster), because we are using this new ARM64 environment we may get **ERROR: Failed building wheel for tokenizers**. If so, we [install Rust](https://huggingface.co/docs/tokenizers/python/v0.9.4/installation/main.html#installation-from-sources) (in the same environment) with:*\n",
    "\n",
    "```bash\n",
    "curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n",
    "```\n",
    "\n",
    "*And then `pip install transformers datasets` again.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the MPS device is available with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.has_mps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Let's pull some data to test the new MPS-enabled PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the first 1K rows of the TREC dataset\n",
    "trec = load_dataset('trec', split='train[:1000]')\n",
    "trec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try loading a BERT model, we'll use this and our TREC data to compare inference time on CPU vs MPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we test inference time using a `batch_size` of `64` on the *CPU*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first 64 rows of the trec data\n",
    "text = trec['text'][:64]\n",
    "# tokenize text using the BERT tokenizer\n",
    "tokens = tokenizer(\n",
    "    text, max_length=512,\n",
    "    truncation=True, padding=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "model(**tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now move model and tokens to MPS and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "tokens.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "model(**tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, although not as good as the release benchmarks would suggest. We can try with a few different batch sizes and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = trec['text'][:256]\n",
    "tokens = tokenizer(\n",
    "    text, max_length=512,\n",
    "    truncation=True, padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "tokens.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "model(**tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try small batches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = trec['text'][:8]\n",
    "tokens = tokenizer(\n",
    "    text, max_length=512,\n",
    "    truncation=True, padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "tokens.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "model(**tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's try this with varying batch sizes and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "reruns = 6\n",
    "b = 10\n",
    "\n",
    "# start with CPU test\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "cpu_times = []\n",
    "\n",
    "for i in range(b):\n",
    "    text = trec['text'][:2**i]\n",
    "    tokens = tokenizer(\n",
    "        text, max_length=512,\n",
    "        truncation=True, padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    tot_time = 0\n",
    "    for _ in range(reruns):\n",
    "        t0 = time()\n",
    "        model(**tokens)\n",
    "        tot_time += time()-t0\n",
    "    cpu_times.append(tot_time/reruns)\n",
    "\n",
    "# then GPU test\n",
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "\n",
    "mps_times = []\n",
    "\n",
    "for i in range(b):\n",
    "    text = trec['text'][:2**i]\n",
    "    tokens = tokenizer(\n",
    "        text, max_length=512,\n",
    "        truncation=True, padding=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    tot_time = 0\n",
    "    for _ in range(reruns):\n",
    "        t0 = time()\n",
    "        model(**tokens)\n",
    "        tot_time += time()-t0\n",
    "    mps_times.append(tot_time/reruns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "sns.lineplot(\n",
    "    x=[2**i for i in range(b)]*2,\n",
    "    y=cpu_times+mps_times,\n",
    "    hue=['cpu']*b + ['mps']*b\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "(numpy.array(cpu_times) - numpy.array(mps_times))/numpy.array(cpu_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5188bc372fa413aa2565ae5d28228f50ad7b2c4ebb4a82c5900fd598adbb6408"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
